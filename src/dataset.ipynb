{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Notebook for remote sensing work\n",
    "Repository made for Artificial Intelligence & Neural Network course with Prof. Ciarfuglia @ Sapienza \n",
    "\n",
    "Import necessary items with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import skimage\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should test that the CUDA platform has been successfully recognized and it's being in use with pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.3.0+cu121  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch has a very useful way of cleaning the dataset, normalizing it and reshaping as needed, using the function classes available in torchvision.transforms()\n",
    "We will be using them to transform our images into Tensors too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.transforms([\n",
    "    transforms.Normalize(mean=0, std=0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset class\n",
    "\n",
    "Now we'll need to create a SN6Dataset derived from data.Dataset that will import the data and return the data and labels, and that implements __len__ and __getitem__ methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "class SN6Dataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                path = \"/data/train/AOI_11_Rotterdam/\", \n",
    "                imgs_path = \"PS-RGB\", \n",
    "                dtype = \"PS-RGB\", \n",
    "                lbls_path = \"geojson_buildings\", \n",
    "                isTrain = True):\n",
    "        super().__init__()\n",
    "        self.imgs_path = os.join(path, imgs_path)\n",
    "        self.lbls_path = os.join(path, lbls_path)\n",
    "        self.dtype = dtype\n",
    "        self.isTrain = isTrain \n",
    "        self.images = os.listdir(imgs_path)\n",
    "        self.labels = os.listdir(lbls_path)\n",
    "        # cose\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.imgs_path, self.images[idx])\n",
    "        lbl_path = os.path.join(self.lbls_path, self.labels[idx])\n",
    "        img = skimage.io.imread(img_path)\n",
    "        label = skimage.io.imread(lbl_path)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader class\n",
    "\n",
    "Now that we have a dataset that imports everything to a class we need to use it for our purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset class\n",
    "\n",
    "Now we'll need to create a SN6Dataset derived from data.Dataset that will import the data and return the data and labels, and that implements __len__ and __getitem__ methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = SN6Dataset()\n",
    "dataset_test = SN6Dataset(path = \"/data/test/AOI_11_Rotterdam/\", imgs_path=\"SAR-Intensity\", dtype=\"SAR-Intensity\", isTrain=False)\n",
    "\n",
    "data_loader = data.DataLoader(dataset_train, batch_size=10, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
