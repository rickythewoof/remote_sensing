{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "To improve the performance of our model, we need to delve into the realm of hyperparameter tuning. Hyperparameters are settings that we can adjust to optimize the behavior and accuracy of our machine learning model, to fine-tune it's performance. \n",
    "\n",
    "The previous model was trained with certain default hyperparameter values, which serve as a baseline. However, these default values may not always yield the best results for our specific problem.\n",
    "\n",
    "Hyperparameter tuning involves systematically exploring different combinations of hyperparameter values to find the optimal configuration that maximizes our model's performance. This process is often iterative and involves training and evaluating the model multiple times with different hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import torch.utils.data as data\n",
    "from rasterio.errors import NotGeoreferencedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning) # Masks are not georeferences, so we can ignore this warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # This will throw a warning message about cudnn, this is normal (https://github.com/pytorch/pytorch/pull/125790)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN = [63.02235933, 66.64201154, 60.63862196]\n",
    "# STD = [55.50368184, 55.35826425, 52.63471437]\n",
    "MEAN = [0, 0, 0]\n",
    "STD = [1.0, 1.0, 1.0]\n",
    "MAX_LR = 1e-4\n",
    "TRAIN_SIZE = 320\n",
    "VAL_SIZE = 360\n",
    "NUM_WORKERS = 4\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VAL_BATCH_SIZE = 4\n",
    "EPOCHS = 80\n",
    "LOAD_BEST = True\n",
    "NAME = \"6 - LR1e-4 TrainBatch8 Epoch80 Resize - NoAugmentation\"\n",
    "\n",
    "DATASET_PATH = \"data/train/AOI_11_Rotterdam/\"\n",
    "OUTPUT_PATH = \"output/\"\n",
    "DATA_PATH = OUTPUT_PATH + \"data/\"\n",
    "CHECKPOINT_PATH = OUTPUT_PATH + \"checkpoints/\"\n",
    "GRAPH_PATH = OUTPUT_PATH + \"graphs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll reimport the default values that we had from before\n",
    "\n",
    "We slightly modified the values of the transformations! We're having a better alignment, and using A.RandomCrop is better than A.Resize because you don't loose details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.set_cuda_and_seed()\n",
    "\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "print(\"CUDA version: \", torch.version.cuda)\n",
    "print(\"cuDNN version: \", torch.backends.cudnn.version())\n",
    "\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.Normalize(mean=MEAN, std=STD, max_pixel_value=255.0),\n",
    "    A.CenterCrop(896, 896),\n",
    "    A.RandomCrop(TRAIN_SIZE, TRAIN_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5)\n",
    "])\n",
    "\n",
    "eval_transforms = A.Compose([\n",
    "    A.Normalize(mean=MEAN, std=STD, max_pixel_value=255.0),\n",
    "    A.RandomCrop(896, 896),\n",
    "    A.Resize(VAL_SIZE, VAL_SIZE)\n",
    "])\n",
    "from dataset import SN6Dataset\n",
    "\n",
    "train_dataset = SN6Dataset(DATASET_PATH, transform=train_transforms, split='train')\n",
    "eval_dataset = SN6Dataset(DATASET_PATH, transform=eval_transforms, split='val')\n",
    "test_dataset = SN6Dataset(DATASET_PATH, transform=eval_transforms, split='test')\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, pin_memory = True, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "eval_loader = data.DataLoader(eval_dataset, pin_memory = True, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = data.DataLoader(test_dataset, pin_memory = True, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "image, mask = next(iter(train_loader))\n",
    "print(f\"Image shape: {image.shape}, Mask shape: {mask.shape}\")\n",
    "utils.visualize_image(image[0], mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be trying something new: A custom loss function that connects both BCEWithLogitLoss and a custom-made Dice loss (since we already established it's our best parameter) in a 50/50 mix. It's been implemented in _custom_loss.py_\n",
    "\n",
    "we'll try to work on \n",
    " - Normalization\n",
    " - A better Segmentation\n",
    " - Maybe slowing down the learning rate even further\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import UNET\n",
    "\n",
    "model = UNET(in_channels = 3, out_channels = 1).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=MAX_LR, weight_decay=1e-5)\n",
    "\n",
    "# We won't use a scheduler at first, but we'll add it later\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=MAX_LR, steps_per_epoch=len(train_loader), epochs=EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things I've noticed in the training is how hard it's for the model to track small buildings, this is because to make our batch_size higher a downscale was necessary; we modified some values, changing:\n",
    " - SIZE to 300x300\n",
    " - BATCH_SIZE to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "\n",
    "best_f1 = 0.0\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'accuracy': [],\n",
    "    'last_epoch': 0 \n",
    "}\n",
    "\n",
    "last_epoch = 0\n",
    "if LOAD_BEST:\n",
    "    print(\"Restoring best model\")\n",
    "    if(os.path.exists(CHECKPOINT_PATH + f\"best_{NAME}.pth\")):\n",
    "        history = utils.load_checkpoint(CHECKPOINT_PATH + f\"best_{NAME}.pth\", model, optimizer, scheduler)\n",
    "        best_f1 = max(history['f1'])\n",
    "        last_epoch = history['last_epoch']\n",
    "        print(f\"Best model found, starting from epoch {last_epoch+1} with f1_score {best_f1}\")\n",
    "    else:\n",
    "        print(\"Best model not found, starting from scratch\")\n",
    "\n",
    "# Training model\n",
    "for epoch in range(last_epoch, EPOCHS):\n",
    "    print(f\"Epoch {epoch+1} of {EPOCHS}\")\n",
    "    train_loss = train(train_loader, model, optimizer, criterion, scaler, scheduler, device)\n",
    "    # Detect if loss is NaN, and immediately stop with a ValueError \n",
    "    \n",
    "    \n",
    "    eval_loss, precision, recall, f1, accuracy = utils.get_evals(eval_loader, model, criterion, device, save_predictions=True, output_path=DATA_PATH)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(eval_loss)\n",
    "    history['precision'].append(precision)\n",
    "    history['recall'].append(recall)\n",
    "    history['f1'].append(f1)\n",
    "    history['accuracy'].append(accuracy)\n",
    "    history['last_epoch'] = epoch\n",
    "    print(f\"Train loss: {train_loss:.4f} Eval loss: {eval_loss:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1: {f1:.4f} Accuracy: {accuracy:.4f}\")\n",
    "    # Save model\n",
    "    checkpoint = {\n",
    "        \"history\" : history,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"loss\": criterion.state_dict(),\n",
    "    }\n",
    "    utils.save_checkpoint(checkpoint, filename=CHECKPOINT_PATH + f\"checkpoint_{NAME}.pth\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        utils.save_checkpoint(checkpoint, filename=CHECKPOINT_PATH + f\"best_{NAME}.pth\")\n",
    "        print(\"Best model saved\")\n",
    "\n",
    "    if epoch - 5 > 0: # Value of validation loss is increasing, model is overfitting. Need to stop training\n",
    "        if history['val_loss'][epoch] > history['val_loss'][epoch-1] > history['val_loss'][epoch-2] > history['val_loss'][epoch-3] > history['val_loss'][epoch-4]:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "print(\"Finished training! Well done :3\\nQuitting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/checkpoints/checkpoint_6 - LR1e-4 TrainBatch8 Epoch80 Resize - NoAugmentation.pth.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get the history\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m image, mask, pred \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_random_image(eval_loader, model, device)\n\u001b[1;32m      6\u001b[0m utils\u001b[38;5;241m.\u001b[39mvisualize_image(image\u001b[38;5;241m.\u001b[39mcpu(), mask\u001b[38;5;241m.\u001b[39mcpu(), pred\u001b[38;5;241m.\u001b[39mcpu(), save_path \u001b[38;5;241m=\u001b[39m GRAPH_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Uni/remote_sensing/./src/utils.py:143\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(name, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_checkpoint\u001b[39m(name, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, criterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m         model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/checkpoints/checkpoint_6 - LR1e-4 TrainBatch8 Epoch80 Resize - NoAugmentation.pth.pth'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the history\n",
    "history = utils.load_checkpoint(CHECKPOINT_PATH + f\"checkpoint_{NAME}.pth\", model)\n",
    "image, mask, pred = utils.get_random_image(eval_loader, model, device)\n",
    "utils.visualize_image(image.cpu(), mask.cpu(), pred.cpu(), save_path = GRAPH_PATH + f\"image_{NAME}.png\")\n",
    "\n",
    "train_loss = history['train_loss']\n",
    "eval_loss = history['val_loss']\n",
    "precision = history['precision']\n",
    "recall = history['recall']\n",
    "accuracy = history['accuracy']\n",
    "f1_score = history['f1']\n",
    "# Plot the loss and other metrics\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Loss')   \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(eval_loss, label='Eval Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Metrics')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Values')\n",
    "plt.plot(precision, label='Precision')\n",
    "plt.plot(recall, label='Recall')\n",
    "plt.plot(accuracy, label='Accuracy')\n",
    "plt.plot(f1_score, label='F1 Score')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(GRAPH_PATH + f\"metrics_{NAME}.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
