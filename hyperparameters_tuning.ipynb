{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "To improve the performance of our model, we need to delve into the realm of hyperparameter tuning. Hyperparameters are settings that we can adjust to optimize the behavior and accuracy of our machine learning model, to fine-tune it's performance. \n",
    "\n",
    "The previous model was trained with certain default hyperparameter values, which serve as a baseline. However, these default values may not always yield the best results for our specific problem.\n",
    "\n",
    "Hyperparameter tuning involves systematically exploring different combinations of hyperparameter values to find the optimal configuration that maximizes our model's performance. This process is often iterative and involves training and evaluating the model multiple times with different hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import torch.utils.data as data\n",
    "from rasterio.errors import NotGeoreferencedWarning\n",
    "warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning) # Masks are not georeferences, so we can ignore this warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # This will throw a warning message about cudnn, this is normal (https://github.com/pytorch/pytorch/pull/125790)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll reimport the default values that we had from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.3.0+cu121  Device: cuda\n",
      "PyTorch version:  2.3.0+cu121\n",
      "CUDA version:  12.1\n",
      "cuDNN version:  8902\n"
     ]
    }
   ],
   "source": [
    "# MEAN = [63.02235933, 66.64201154, 60.63862196]\n",
    "# STD = [55.50368184, 55.35826425, 52.63471437]\n",
    "MEAN = [0, 0, 0]\n",
    "STD = [1.0, 1.0, 1.0]\n",
    "INITIAL_LR = 1e-3\n",
    "MAX_LR = 1e-2\n",
    "SIZE = 200\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 16\n",
    "LOAD_BEST = True\n",
    "\n",
    "# Let's define some paths\n",
    "DATASET_PATH = \"data/train/AOI_11_Rotterdam/\"\n",
    "OUTPUT_PATH = \"output/\"\n",
    "CHECKPOINT_PATH = OUTPUT_PATH + \"checkpoints/\"\n",
    "GRAPH_PATH = OUTPUT_PATH + \"graphs/\"\n",
    "\n",
    "device = utils.set_cuda_and_seed()\n",
    "\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "print(\"CUDA version: \", torch.version.cuda)\n",
    "print(\"cuDNN version: \", torch.backends.cudnn.version())\n",
    "\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.Normalize(mean=MEAN, std=STD, max_pixel_value=255.0),\n",
    "    A.Resize(SIZE, SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5)\n",
    "])\n",
    "\n",
    "eval_transforms = A.Compose([\n",
    "    A.Normalize(mean=MEAN, std=STD, max_pixel_value=255.0),\n",
    "    A.Resize(SIZE, SIZE)\n",
    "])\n",
    "\n",
    "from dataset import SN6Dataset\n",
    "\n",
    "train_dataset = SN6Dataset(DATASET_PATH, transform=train_transforms, split='train')\n",
    "eval_dataset = SN6Dataset(DATASET_PATH, transform=eval_transforms, split='val')\n",
    "test_dataset = SN6Dataset(DATASET_PATH, transform=eval_transforms, split='test')\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "eval_loader = data.DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import UNET\n",
    "from dataset import SN6Dataset\n",
    "\n",
    "model = UNET(3, 1).to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=INITIAL_LR)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=INITIAL_LR, max_lr=MAX_LR, step_size_up=2 * len(train_loader))\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "train_dataset = SN6Dataset(DATASET_PATH, transform=train_transforms, split='train')\n",
    "eval_dataset = SN6Dataset(DATASET_PATH, transform=eval_transforms, split='val')\n",
    "test_dataset = SN6Dataset(DATASET_PATH, transform=eval_transforms, split='test')\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "eval_loader = data.DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0.0\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'accuracy': []\n",
    "}\n",
    "\n",
    "last_epoch = 0\n",
    "if LOAD_BEST:\n",
    "    print(\"Restoring best model\")\n",
    "    if(os.path.exists(CHECKPOINT_PATH + \"best.pth\")):\n",
    "        history, last_epoch = utils.load_checkpoint(CHECKPOINT_PATH + \"best.pth\", model, optimizer, scheduler)\n",
    "        best_f1 = max(history['f1'])\n",
    "    else:\n",
    "        print(\"Best model not found, starting from scratch\")\n",
    "\n",
    "# Training model\n",
    "for epoch in range(last_epoch, EPOCHS):\n",
    "    print(f\"Epoch {epoch+1} of {EPOCHS}\")\n",
    "    train_loss = train(train_loader, model, optimizer, criterion, scaler, scheduler, device)\n",
    "    # Detect if loss is NaN, and immediately stop with a ValueError \n",
    "    if train_loss != train_loss:\n",
    "        raise ValueError(\"Loss is NaN, something is VERY wrong, stopping training\")\n",
    "    \n",
    "    eval_loss, precision, recall, f1, accuracy = utils.get_evals(eval_loader, model, criterion, device, save_predictions=True, output_path=\"data/eval\")\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(eval_loss)\n",
    "    history['precision'].append(precision)\n",
    "    history['recall'].append(recall)\n",
    "    history['f1'].append(f1)\n",
    "    history['accuracy'].append(accuracy)\n",
    "    print(f\"Train loss: {train_loss:.4f} Eval loss: {eval_loss:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1: {f1:.4f} Accuracy: {accuracy:.4f}\")\n",
    "    # Save model\n",
    "    checkpoint = {\n",
    "        \"history\" : history,\n",
    "        \"epoch\" : epoch,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"loss\": criterion.state_dict(),\n",
    "    }\n",
    "    utils.save_checkpoint(checkpoint, filename=CHECKPOINT_PATH + \"checkpoint.pth\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        utils.save_checkpoint(checkpoint, filename=CHECKPOINT_PATH + \"best.pth\")\n",
    "        print(\"Best model saved\")\n",
    "\n",
    "    if epoch - 3 > 0: # Value of validation loss is increasing, model is overfitting. Need to stop training\n",
    "        if history['val_loss'][epoch] > history['val_loss'][epoch-1] > history['val_loss'][epoch-2]:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "print(\"Finished training! Well done :3\\nQuitting...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
